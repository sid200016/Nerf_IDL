expname = flower_test_high_quality
basedir = Nerf_IDL/logs
datadir = Nerf_IDL/data/nerf_llff_data/flower
dataset_type = llff

# High resolution training (factor 4 = 2x resolution, factor 2 = 4x resolution)
factor = 4  # Reduced from 8 for higher resolution (2x more pixels)
llffhold = 8

# Increased sampling for better quality
N_rand = 1024  # Increased batch size for more stable gradients (if memory allows, else 1024)
N_samples = 128  # More coarse samples per ray (doubled from 64)
N_importance = 128  # More fine samples per ray (doubled from 64)

use_viewdirs = True
# Regularization - standard NeRF setting
raw_noise_std = 1e-3

# Load weights from fixed-encoding baseline (warmup strategy)
# Set this to the path of your baseline checkpoint, e.g.:
# ft_path = ./logs/flower_baseline/200000.tar
# ft_path = None  # Set to None if training from scratch
ft_path = None

# Network architecture (wider backbone for better PSNR)
netwidth = 512  # Increased from 256 for more capacity
netwidth_fine = 512  # Fine network width
netdepth = 8
netdepth_fine = 8

# Learnable Fourier Encoding (optimized for maximum PSNR)
learnable_pe = True

# Frequency bands - key parameter for PSNR
# Match the fixed-encoding baseline for fair comparison
multires = 14  # Same band count as the fixed baseline
multires_views = 6  # Restored to baseline view-dependent capacity

# Fourier Encoding Hyperparameters (based on commit fc8f4840 - similar to original NeRF)
pe_learnable_freqs = True  # Make frequency bands learnable (default: True)
pe_use_gating = True  # Use per-frequency amplitude gates (default: False, set True for soft progressive encoding)
pe_init_scale = 1.0  # Initial scale for frequency initialization (default: 1.0)
pe_lr_scale = 0.01  # PE learning rate relative to network LR (increased for faster adaptation)
learnable_pe_phase = True  # Make phase shifts learnable (enabled for more flexibility)

# Learning rate (increased for faster convergence - similar to commit that worked)
lrate = 5e-4

# Learning rate decay (slower decay to maintain learning longer - from working commit)
lrate_decay = 500

# Gradient clipping (reduced to allow larger updates - from working commit)
grad_clip = 1.0

# Weights & Biases logging
use_wandb = True
wandb_project = nerf-idl
wandb_run_name = flower_pe_grad-new_model 


